@INPROCEEDINGS{GEDF,
author={S. {Kato} and Y. {Ishikawa}},
booktitle={2009 30th IEEE Real-Time Systems Symposium},
title={Gang EDF Scheduling of Parallel Task Systems},
year={2009},
volume={},
number={},
pages={459-468},
keywords={parallel processing;processor scheduling;gang EDF scheduling;parallel task systems;preemptive real-time scheduling;sporadic parallel task systems;earliest deadline first policy;interference;sequential task model;multiprocessor systems;parallel processing;Yarn;Real time systems;Processor scheduling;Parallel processing;Scheduling algorithm;Interference;Testing;Multiprocessing systems;Information science;Concurrent computing;Earliest Deadline First;Gang Scheduling;Parallel Processing;Real-Time Systems},
doi={10.1109/RTSS.2009.42},
ISSN={1052-8725},
month={Dec},}

@url{WPNV,
author = {White Paper NVIDIA},
title = {{NVIDIA Tesla P100: The Most Advanced Datacenter Accelerator Ever Built}},
url = {https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf}
}
@url{gpgpu,
author = {NVIDIA},
title = {{COMPUTACI{\'{O}}N ACELERADA: Supera los desaf{\'{i}}os m{\'{a}}s importantes del mundo}},
url = {https://la.nvidia.com/object/what-is-gpu-computing-la.html}
}

@article{Buttazzo2013,
abstract = {The question whether preemptive algorithms are better than nonpreemptive ones for scheduling a set of real-time tasks has been debated for a long time in the research community. In fact, especially under fixed priority systems, each approach has advantages and disadvantages, and no one dominates the other when both predictability and efficiency have to be taken into account in the system design. Recently, limited preemption models have been proposed as a viable alternative between the two extreme cases of fully preemptive and nonpreemptive scheduling. This paper presents a survey of the existing approaches for reducing preemptions and compares them under different metrics, providing both qualitative and quantitative performance evaluations. {\textcopyright} 2005-2012 IEEE.},
author = {Buttazzo, Giorgio C. and Bertogna, Marko and Yao, Gang},
doi = {10.1109/TII.2012.2188805},
file = {:Users/antonioayala/Downloads/buttazzo2013.pdf:pdf},
issn = {15513203},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Limited-preemptive scheduling,nonpreemptive regions,real-time systems},
number = {1},
pages = {3--15},
title = {{Limited preemptive scheduling for real-time systems. A survey}},
volume = {9},
year = {2013}
}
@misc{Slurm,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Nathan, Andrew J. and Scobell, Andrew},
booktitle = {Foreign Affairs},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {00157120},
keywords = {icle},
number = {5},
pages = {1689--1699},
pmid = {25246403},
title = {{How China sees America}},
url = {https://slurm.schedmd.com/documentation.html},
volume = {91},
year = {2012}
}
@article{Liu,
abstract = {The problem of multiprogram scheduling on a single processor is studied from the viewpoint of the characteristics peculiar to the program functions that need guaranteed service. It is shown that an optimum fixed priority scheduler possesses an upper bound to processor utilization which may be as low as 70 percent for large task sets. It is also shown that full processor utilization can be achieved by dynamically assigning priorities on the basis of their current deadlines. A combination of these two scheduling techniques is also discussed. {\textcopyright} 1973, ACM. All rights reserved.},
author = {Liu, C. L. and Layland, James W.},
doi = {10.1145/321738.321743},
issn = {1557735X},
journal = {Journal of the ACM (JACM)},
keywords = {deadline driven scheduling,dynamic scheduling,multiprogram scheduling,priority assignment,processor utilization,real-time multiprogramming,scheduling},
number = {1},
pages = {46--61},
title = {{Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment}},
volume = {20},
year = {1973}
}
@url{PinnM,
author = {Overhead, Memory Transfer},
title = {{Choosing Between Pinned and Non-Pinned Memory :}},
url = {https://www.cs.virginia.edu/{~}mwb7w/cuda{\_}support/pinned{\_}tradeoff.html}
}
@booklet{mygpu,
author = {Anders, Jacob},
title = {{Matrix computations on the GPU (CUBLAS Examples)}},
year = {2017}
}
@url{AEC2,
author = {Amazon},
keywords = {Amazon EC2,EC2,EC2-Instance,Instance,VM,Virtu},
pages = {659},
title = {{Amazon Elastic Compute Cloud Documentation}},
url = {http:// aws.amazon.com/ec2/},
year = {2018}
}
@booklet{matmulCu,
abstract = {BACKGROUND: Fat-free mass (FFM) depletion marks the imbalance between tissue protein synthesis and breakdown in chronic obstructive pulmonary disease (COPD). To date, the role of essential amino acid supplementation (EAAs) in FFM repletion has not been fully acknowledged. A pilot study was undertaken in patients attending pulmonary rehabilitation. METHODS: 28 COPD patients with dynamic weight loss {\textgreater} 5{\%} over the last 6 months were randomized to receive EAAs embedded in a 12-week rehabilitation program (EAAs group n = 14), or to the same program without supplementation (C group n = 14). Primary outcome measures were changes in body weight and FFM, using dual X-ray absorptiometry (DEXA). RESULTS: At the 12th week, a body weight increment occurred in 92{\%} and 15{\%} of patients in the EAAs and C group, respectively, with an average increase of 3.8 +/- 2.6 kg (P = 0.0002) and -0.1 +/- 1.1 kg (P = 0.81), respectively. A FFM increment occurred in 69{\%} and 15{\%} of EAAs and C patients, respectively, with an average increase of 1.5 +/- 2.6 kg (P = 0.05) and -0.1 +/- 2.3 kg (P = 0.94), respectively. In the EAAs group, FFM change was significantly related to fasting insulin (r(2) 0.68, P {\textless} 0.0005), C-reactive protein (C-RP) (r(2) = 0.46, P {\textless} 0.01), and oxygen extraction tension (PaO(2x)) (r(2) = 0.46, P {\textless} 0.01) at end of treatment. These three variables were highly correlated in both groups (r {\textgreater} 0.7, P {\textless} 0.005 in all tests). CONCLUSIONS: Changes in FFM promoted by EAAs are related to cellular energy and tissue oxygen availability in depleted COPD. Insulin, C-RP, and PaO(2x) must be regarded as clinical markers of an amino acid-stimulated signaling to FFM accretion},
author = {Hochberg, Robert},
booktitle = {Tech. Rep.},
pmid = {20368909},
title = {{Matrix Multiplication with CUDA — A basic introduction to the CUDA programming model}},
year = {2012}
}
@url{jtx2dk,
abstract = {Many clustering techniques have been proposed for the analysis of gene expression data obtained from microarray experiments. However, choice of suitable method(s) for a given experimental dataset is not straightforward. Common approaches do not translate well and fail to take account of the data profile. This review paper surveys state of the art applications which recognise these limitations and addresses them. As such, it provides a framework for the evaluation of clustering in gene expression analyses. The nature of microarray data is discussed briefly. Selected examples are presented for clustering methods considered.},
author = {plawrence Jsachs},
doi = {10.1016/j.compbiomed.2007.11.001},
issn = {0010-4825},
pages = {1--24},
pmid = {18061589},
title = {{Jetson TX2 Developer Kit}},
url = {https://developer.nvidia.com/embedded/jetson-tx2-developer-kit},
year = {2017}
}
@url{TX2I,
abstract = {NVIDIA today unveiled the NVIDIA{\textregistered} Jetson™ TX2, a credit card-sized platform that delivers AI computing at the edge -- opening the door to powerfully intelligent factory robots, commercial drones and smart cameras for AI cities. Jetson TX2 offers twice the performance of its predecessor, or it can run at more than twice the power efficiency, while drawing less than 7.5 watts of power. This allows Jetson TX2 to run larger, deeper neural networks on edge devices. The result: smarter devices with higher accuracy and faster response times for tasks like image classification, navigation and speech recognition. "Jetson TX2 brings powerful AI capabilities at the edge, making possible a new class of intelligent machines," said Deepu Talla, vice president and general manager of the Tegra business at NVIDIA. "These devices will enable intelligent video analytics that keep our cities smarter and safer, new kinds of robots that optimize manufacturing, and new collaboration that makes long-distance work more efficient." The Jetson TX2 joins the Jetson TX1 and TK1 products for embedded computing. Jetson is an open platform that is accessible to anyone for developing advanced AI solutions at the edge -- from enterprise companies and startups to researchers and high school students. - See more at: http://nvidianews.nvidia.com/news/nvidia-jetson-tx2-enables-ai-at-the-edge{\#}sthash.FObdytfZ.dpuf},
author = {{Kristin Uchiyama}},
title = {{NVIDIA Jetson TX2 Enables AI at the Edge | NVIDIA Newsroom}},
url = {http://nvidianews.nvidia.com/news/nvidia-jetson-tx2-enables-ai-at-the-edge},
year = {2017}
}
@url{ArqTX2,
abstract = {Blog post regarding the launch of the nVidia Jetson TX2.},
author = {Franklin, Dustin},
booktitle = {Nvidia Developer Blog},
title = {{NVIDIA Jetson TX2 Delivers Twice the Intelligence to the Edge}},
url = {https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/},
year = {2017}
}

@article{NPr,
abstract = {Real-time systems are often designed using preemptive scheduling and worst-case execution time estimates to guarantee the execution of high priority tasks. There is, however, an interest in exploring non-preemptive scheduling models for real-time systems, particularly for soft real-time multimedia applications. In this paper, we propose a new algorithm that uses multiple scheduling strategies for efficient non-preemptive scheduling of tasks. Our goal is to improve the success ratio of the well-known Earliest Deadline First (EDF) approach when the load on the system is very high and to improve the overall performance in both underloaded and overloaded conditions. Our approach, known as group-EDF (gEDF) is based on dynamic grouping of tasks with deadlines that are very close to each other, and using Shortest Job First (SJF) technique to schedule tasks within the group. We will present results comparing gEDF with other real-time algorithms including, EDF, Best-effort, and Guarantee, by using randomly generated tasks with varying execution times, release times, deadlines and tolerance to missing deadlines, under varying workloads. We believe that grouping tasks dynamically with similar deadlines and utilizing a secondary criteria, such as minimizing the total execution time (or other metrics such as power or resource availability) for scheduling tasks within a group, can lead to new and more efficient real-time scheduling algorithms. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Li, Wenming and Kavi, Krishna and Akl, Robert},
doi = {10.1016/j.compeleceng.2006.04.002},
issn = {00457906},
journal = {Computers and Electrical Engineering},
keywords = {Best-effort scheduling,Earliest Deadline First (EDF),Group-EDF,Non-preemptive real-time scheduling,Shortest Job First (SJF),Soft real-time systems},
number = {1},
pages = {12--29},
title = {{A non-preemptive scheduling algorithm for soft real-time systems}},
volume = {33},
year = {2007}
}
@article{TX2-H,
abstract = {The push towards fielding autonomous-driving capabilities in vehicles is happening at breakneck speed. Semi-autonomous features are becoming increasingly common, and fully autonomous vehicles are optimistically forecast to be widely available in just a few years. Today, graphics processing units (GPUS) are seen as a key technology in this push towards greater autonomy. However, realizing full autonomy in mass-production vehicles will necessitate the use of stringent certification processes. Currently available GPUS pose challenges in this regard, as they tend to be closed-source 'black boxes' that have features that are not publicly disclosed. For certification to be tenable, such features must be documented. This paper reports on such a documentation effort. This effort was directed at the NVIDIA TX2, which is one of the most prominent GPU-enabled platforms marketed today for autonomous systems. In this paper, important aspects of the TX2's GPU scheduler are revealed as discerned through experimental testing and validation.},
author = {Amert, Tanya and Otterness, Nathan and Yang, Ming and Anderson, James H. and {Donelson Smith}, F.},
doi = {10.1109/RTSS.2017.00017},
isbn = {9781538614143},
issn = {10528725},
journal = {Proceedings - Real-Time Systems Symposium},
keywords = {Embedded-software,Graphics-processing-units,Parallel-computing,Real-time-systems,Scheduling-algorithms},
pages = {104--115},
title = {{GPU Scheduling on the NVIDIA TX2: Hidden Details Revealed}},
volume = {2018-January},
year = {2017}
}
@misc{stream,
abstract = {Nice Presentation of concurrency},
author = {Rennich, Steve},
booktitle = {NVIDIA,[online]},
institution = {NVIDIA},
title = {{Cuda c/c++ streams and concurrency}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:CUDA+C+/+C++++Streams+and+Concurrency{\#}0{\%}5Cnhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Cuda+c/c+++streams+and+concurrency{\#}0},
year = {2012}
}
@article{IntraNode,
abstract = {GPUs in High-Performance Computing systems remain under-utilised due to the unavailability of schedulers that can safely schedule multiple applications to share the same GPU. The research reported in this paper is motivated to improve the utilisation of GPUs by proposing a framework, we refer to as schedGPU, to facilitate intra-node GPU co-scheduling such that a GPU can be safely shared among multiple applications by taking memory constraints into account. Two approaches, namely a client-server and a shared memory approach are explored. However, the shared memory approach is more suitable due to lower overheads when compared to the former approach. Four policies are proposed in schedGPU to handle applications that are waiting to access the GPU, two of which account for priorities. The feasibility of schedGPU is validated on three real-world applications. The key observation is that a performance gain is achieved. For single applications, a gain of over 10 times, as measured by GPU utilisation and GPU memory utilisation, is obtained. For workloads comprising multiple applications, a speed-up of up to 5x in the total execution time is noted. Moreover, the average GPU utilisation and average GPU memory utilisation is increased by 5 and 12 times, respectively.},
archivePrefix = {arXiv},
arxivId = {1712.04495},
author = {Reano, Carlos and Silla, Federico and Nikolopoulos, Dimitrios S. and Varghese, Blesson},
doi = {10.1109/TPDS.2017.2784428},
eprint = {1712.04495},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {GPU co-scheduling,accelerator,access synchronisation,memory safe,schedGPU,under-utilisation},
number = {5},
pages = {1089--1102},
title = {{Intra-Node Memory Safe GPU Co-Scheduling}},
volume = {29},
year = {2018}
}
@book{EmbSysDes,
abstract = {In this chapter, we discussed the design process, related tools, and applied the process to a real world design. As previously mentioned, this design example will be periodically revisited throughout the text. It is essential to follow a systematic, disciplined approach to embedded systems design to successfully develop a prototype that meets established requirements.},
author = {Heath, Steve},
booktitle = {Synthesis Lectures on Digital Circuits and Systems},
doi = {10.1201/9781482276824-10},
isbn = {9781608451289},
issn = {19323166},
pages = {1--14},
publisher = {EDN Series For Design Engineers},
title = {{Embedded systems design}},
volume = {24},
year = {2009}
}
@article{RM,
abstract = {An exact characterization of the ability of the rate monotonic scheduling algorithm to meet the deadlines of a periodic task set is represented. In addition, a stochastic analysis which gives the probability distribution of the breakdown utilization of randomly generated task sets is presented. It is shown that as the task set size increases, the task computation times become of little importance, and the breakdown utilization converges to a constant determined by the task periods. For uniformly distributed tasks, a breakdown utilization of 88{\%} is a reasonable characterization. A case is shown in which the average-case breakdown utilization reaches the worst-case lower bound of C. L. Liu and J. W. Layland (1973).},
author = {Lehoczky, John and Sha, Lui and Ding, Ye},
doi = {10.1109/real.1989.63567},
isbn = {0818620048},
journal = {Proceedings - Real-Time Systems Symposium},
pages = {166--171},
title = {{Rate monotonic scheduling algorithm: Exact characterization and average case behavior}},
year = {1989}
}
@article{GPUArt,
abstract = {Emerging technologies like autonomous driving entail computational intense software solutions. More and more companies accelerate their embedded applications by General Purpose Computing on the Graphics Processing Unit (GPGPU), in order to overcome those computational demands. Unfortunately, Graphics Processing Units (GPUs) severely lack real-time capability, for example controllable preemption support, which limits their applicability in the embedded domain. We therefore present GPUart, a framework for GPU real-time scheduling. GPUart focuses on embedded systems and requires neither hardware nor driver stack extensions. We propose a software-only approach for preemption, based on the fixed preemption point strategy. In contrast to prior work, GPUart enables preemption inside a thread block by adding fixed preemption points. We further propose a portable high-level resource management concept to enable gang scheduling on GPUs. GPUart can schedule GPU workload either under the Gang-Earliest Deadline First (EDF) or Gang-Fixed Task Priority (FTP) policy. A case-study on Nvidia Tegra X1, using real-world engine management applications from Audi AG and Continental Automotive GmbH, shows that only up to 0.28{\%} additional global memory is required to enable interruptible thread blocks. GPUart reduces the worst observed response times by a factor of up to 221, leading to response times without deadline misses.},
author = {Hartmann, Christoph and Margull, Ulrich},
doi = {10.1016/j.sysarc.2018.10.005},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {Automotive,Embedded systems,GPU resource management,Graphics processing unit (GPU),Limited preemption,Real-time scheduling},
pages = {304--319},
title = {{GPUart - An application-based limited preemptive GPU real-time scheduler for embedded systems}},
volume = {97},
year = {2019}
}
@book{Buta2011,
abstract = {Hard Real-Time Computing Systems: Predictable Scheduling Algorithms and Applications is a basic treatise on real-time computing, with particular emphasis on predictable scheduling algorithms. It introduces the fundamental concepts of real-time computing, illustrates the most significant results in the field, and provides the essential methodologies for designing predictable computing systems which can be used to support critical control applications. This volume serves as a textbook for advanced level courses on the topic. Each chapter provides basic concepts, which are followed by algorithms that are illustrated with concrete examples, figures and tables. Exercises are included with each chapter and solutions are given at the end of the book. The book also provides an excellent reference for those interested in real-time computing for designing and/or developing predictable control applications.},
author = {Butazzo, Giorgio C},
booktitle = {Computers {\&} Mathematics with Applications},
doi = {10.1016/s0898-1221(98)90205-x},
issn = {08981221},
number = {3},
pages = {126},
publisher = {Springer Science {\&} Business Media},
title = {{Hard real-time computing systems: Predictable scheduling algorithms and applications}},
volume = {36},
year = {1998}
}
@article{8715778,
abstract = {The earliest deadline first (EDF) scheduling algorithm is a typical representative of the dynamic priority scheduling algorithm. However, once the system is overloaded, the deadline miss rate increases and the scheduling performance deteriorates sharply, which causes a reduction in system resource utilization. To overcome this problem, we proposed an improved dynamic priority scheduling algorithm based on heap sorting. The task deadline, task value, energy consumption, and other parameters were introduced. The fuzzy analytic hierarchy process (FAHP) and the value density method were then used to determine the comprehensive priority of tasks. A heapsort algorithm with a lower time complexity was used to sort the comprehensive priority index so as to reduce the sorting overhead of the system. The system sorting overhead was then introduced to improve the decision condition of the priority scheduling subset and expand the schedulable range of the algorithm. The experimental results showed that the improved method reduced the deadline miss rate by an average of 0.1789, which improved the scheduling performance of the algorithm. The optimized scheduling algorithm can be applied to industrial control to improve system efficiency, and reasonable resource scheduling can reduce data center costs.},
author = {Meng, Shanshan and Zhu, Qiang and Xia, Fei},
doi = {10.1109/ACCESS.2019.2917043},
issn = {21693536},
journal = {IEEE Access},
keywords = {EDF,FAHP,Real-time operating system,heapsort,value density},
pages = {68503--68510},
title = {{Improvement of the Dynamic Priority Scheduling Algorithm Based on a Heapsort}},
volume = {7},
year = {2019}
}
@manual{NCUDA,
abstract = {Graphics processing units (GPUs) can provide excellent speedups on some, but not all, general-purpose workloads. Using a set of computational GPU kernels as examples, the authors show how to adapt kernels to utilize the architectural features of a GeForce 8800 GPU and what finally limits the achievable performance.},
author = {Hwu, Wen-Mei},
booktitle = {Computing in Science Engineering},
doi = {10.1109/MCSE.2009.48},
edition = {Version 2.},
issn = {15219615},
number = {3},
organization = {NVIDIA},
pages = {16--26},
title = {{NVIDIA CUDA Compute Unified Device Architecture}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4814979},
volume = {11},
year = {2009}
}
@book{jitter,
abstract = {Timing jitter is the unwelcome companion of all electrical systems that use voltage transitions to represent timing information. Historically, electrical systems have lessened the ill effects of timing jitter (or, simply “jitter”) by employing relatively low signaling rates. As a consequence, jitter-induced errors have been small when compared with the time intervals that they corrupt. The timing margins associated with today's high-speed serial buses and data links reveal that a tighter control of jitter is needed throughout the system design. As signaling rates climb above 2 GHz and voltage swings shrink to conserve power, the timing jitter in a system becomes a significant percentage of the signaling interval. Under these circumstances, jitter becomes a fundamental performance limit. Understanding what jitter is, and how to characterize it, is the first step to successfully deploying high-speed systems that dependably meet their performance requirements. A more thorough definition will be introduced in Section 2, but conceptually, jitter is the deviation of timing edges from their “correct” locations. In a timing-based system, timing jitter is the most obvious and direct form of non-idealness. As a form of noise, jitter must be treated as a random process and characterized in terms of its statistics. If you have a way to measure jitter statistics, you can compare components and systems to each other and to chosen limits. However, this alone will not allow you to efficiently refine and debug a cutting-edge design. Only by thoroughly analyzing jitter is it possible for the root causes to be isolated, so that they can be reduced systematically rather than by trial and error. This analysis takes the form of jitter visualization and decomposition, discussed in detail in Sections 3 and 4. Although there are many similarities between the causes, behavior and characterization of electrical and optical jitter, the equipment used to measure jitter in optical systems differs from that used in electrical systems. This paper focuses primarily on jitter in electrical systems.},
author = {Tektronix},
booktitle = {Time},
pages = {1--16},
publisher = {Tektronix Inc. Firmenschrift.},
title = {{Understanding and Characterizing Timing Jitter}},
year = {2007}
}
@book{enSWE,
author = {{Salvador S{\'{a}}nchez, Miguel {\'{A}}ngel Sicilia}, Daniel Rodriguez},
pages = {267},
publisher = {Alfaomega Grupo Editor, S.A. de C.V},
title = {{Ingenieria del software: un enfoque desde la gu{\'{i}}a SWEBOK.}},
year = {2012}
}
@article{8,
abstract = {Context switching is a key technique enabling preemption and time-multiplexing for CPUs. However, for single-instruction multiple-thread (SIMT) processors such as high-end graphics processing units (GPUs), it is challenging to support context switching due to the massive number of threads, which leads to a huge amount of architectural states to be swapped during context switching. The architectural state of SIMT processors includes registers, shared memory, SIMT stacks and barrier states. Recent works present thread-block-level preemption on SIMT processors to avoid context switching overhead. However, because the execution time of a thread block (TB) is highly dependent on the kernel program. The response time of preemption cannot be guaranteed and some TB-level preemption techniques cannot be applied to all kernel functions. In this paper, we propose three complementary ways to reduce and compress the architectural states to achieve lightweight context switching on SIMT processors. Experiments show that our approaches can reduce the register context size by 91.5{\%} on average. Based on lightweight context switching, we enable instruction-level preemption on SIMT processors with compiler and hardware co-design. With our proposed schemes, the preemption latency is reduced by 59.7{\%} on average compared to the naive approach.},
author = {Lin, Zhen and Nyland, Lars and Zhou, Huiyang},
doi = {10.1109/SC.2016.76},
isbn = {9781467388153},
issn = {21674337},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
keywords = {computer architecture;flip-flops;graphics processi},
month = {nov},
pages = {898--908},
title = {{Enabling Efficient Preemption for SIMT Architectures with Lightweight Context Switching}},
volume = {0},
year = {2016}
}
@manual{nvi,
author = {Ko, Marcin},
month = {sep},
title = {{nVidia Hardware Documentation}},
year = {2014}
}
@article{20,
abstract = {Studies show that non-graphics programs can be less optimized for the GPU hardware, leading to significant resource under-utilization. Sharing the GPU among multiple programs can effectively improve utilization, which is particularly attractive to systems where many applications require access to the GPU (e.g., cloud computing). However, current GPUs lack proper architecture features to support sharing. Initial attempts are preliminary: They either provide only static sharing, which requires recompilation or code transformation, or they do not effectively improve GPU resource utilization. We propose Simultaneous Multikernel (SMK), a fine-grain dynamic sharing mechanism, that fully utilizes resources within a streaming multiprocessor by exploiting heterogeneity of different kernels. We propose several resource allocation strategies to improve system throughput while maintaining fairness. Our evaluation shows that for shared workloads with complementary resource occupancy, SMK improves GPU throughput by 52{\%} over non-shared execution and 17{\%} over a state-of-the-art design.},
author = {Wang, Zhenning and Yang, Jun and Melhem, Rami and Childers, Bruce and Zhang, Youtao and Guo, Minyi},
doi = {10.1109/HPCA.2016.7446078},
isbn = {9781467392112},
issn = {15300897},
journal = {Proceedings - International Symposium on High-Performance Computer Architecture},
keywords = {graphics processing units;resource allocation;simu},
month = {mar},
pages = {358--369},
title = {{Simultaneous Multikernel GPU: Multi-tasking throughput processors via fine-grained sharing}},
volume = {2016-April},
year = {2016}
}
@article{19,
abstract = {The demand for multitasking on graphics processing units (GPUs) is constantly increasing as they have become one of the default components on modern computer systems along with traditional processors (CPUs). Preemptive multitasking on CPUs has been primarily supported through context switching. However, the same preemption strategy incurs substantial overhead due to the large context in GPUs. The overhead comes in two dimensions: a preempting kernel suffers from a long preemption latency, and the system throughput is wasted during the switch. Without precise control over the large preemption overhead, multitasking on GPUs has little use for applications with strict latency requirements. In this paper, we propose Chimera, a collaborative preemption approach that can precisely control the overhead for multitasking on GPUs. Chimera first introduces streaming multiprocessor (SM) flushing, which can instantly preempt an SM by detecting and exploiting idempotent execution. Chimera utilizes flushing collaboratively with two previously proposed preemption techniques for GPUs, namely context switching and draining to minimize throughput overhead while achieving a required preemption latency. Evaluations show that Chimera violates the deadline for only 0.2{\%} of preemption requests when a 15$\mu$s preemption latency constraint is used. For multi-programmed workloads, Chimera can improve the average normalized turnaround time by 5.5x, and system throughput by 12.2{\%}.},
address = {New York, NY, USA},
author = {Park, Jason Jong Kyu and Park, Yongjun and Mahlke, Scott},
doi = {10.1145/2694344.2694346},
isbn = {9781450328357},
issn = {15232867},
journal = {ACM SIGPLAN Notices},
keywords = {Context switch,Graphics processing unit,Idempotence,Preemptive multitasking},
number = {4},
pages = {593--606},
publisher = {Association for Computing Machinery},
series = {ASPLOS '15},
title = {{Chimera: Collaborative preemption for multitasking on a shared GPU}},
url = {https://doi.org/10.1145/2694344.2694346},
volume = {50},
year = {2015}
}
@book{ULinux,
abstract = {Title: Understanding The LINUX Kernel 3rd Ed SubTitle: From I/O Ports To Process Management ; Volume: ; Serie: ; Edition: ; Authors: Bovet, Daniel P. And Cesati, Marco ; Year: 2006 ; Pages: 944 ; Editor: ; Publisher: O'Reilly ; ISBN: 978-0-596-00565-8 ; Keywords: ;},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bovet, By Daniel P and Cesati, Marco},
booktitle = {Journal of Enhanced Heat Transfer},
doi = {10.1615/JEnhHeatTransf.v12.i4.80},
eprint = {arXiv:1011.1669v3},
isbn = {0596005652},
issn = {1065-5131},
number = {4},
pages = {395--402},
pmid = {21257348},
publisher = {Oreilly {\&} Associates Inc.},
title = {{Understanding the Linux Kernel, 3rd Edition}},
url = {http://www.begellhouse.com/journals/4c8f5faa331b09ea,5bb1e40a370b37c7,501c7b0a29b032c3.html},
volume = {12},
year = {2005}
}
@book{Tanenbaum,
abstract = {For software development professionals and computer science students, Modern Operating Systems gives a solid conceptual overview of operating system design, including detailed case studies of Unix/Linux and Windows 2000. Readers familiar with Tanenbaum's previous text, Operating Systems, know the author is a great proponent of simple design and hands-on experimentation. His earlier book came bundled with the source code for an operating system called Minux, a simple variant of Unix and the platform used by Linus Torvalds to develop Linux. Although this book does not come with any source code, he illustrates many of his points with code fragments (C, usually with Unix system calls). The first half of Modern Operating Systems focuses on traditional operating systems concepts: processes, deadlocks, memory management, I/O, and file systems. There is nothing ground-breaking in these early chapters, but all topics are well covered, each including sections on current research and a set of student problems. It is the second half of the book that differentiates itself from older operating systems texts. Here, each chapter describes an element of what constitutes a modern operating system-awareness of multimedia applications, multiple processors, computer networks, and a high level of security. The chapter on multimedia functionality focuses on such features as handling massive files and providing video-on-demand. Included in the discussion on multiprocessor platforms are clustered computers and distributed computing. Finally, the importance of security is discussed-a lively enumeration of the scores of ways operating systems can be vulnerable to attack, from password security to computer viruses and Internet worms. Included at the end of the book are case studies of two popular operating systems: Unix/Linux and Windows 2000. There is a bias toward the Unix/Linux approach, not surprising given the author's experience and academic bent, but this bias does not detract from Tanenbaum's analysis. Both operating systems are dissected, describing how each implements processes, file systems, memory management, and other operating system fundamentals. Tanenbaum's mantra is a simple, accessible operating system design. Given that modern operating systems have extensive features, he is forced to reconcile physical size with simplicity. Towards this end, he makes frequent references to the Frederick Brooks classic The Mythical Man Month for wisdom on managing large, complex software development projects. He finds both Windows 2000 and Unix/Linux guilty of being too complicated-with a particular skewering of Windows 2000 and its "mammoth Win32 API". A primary culprit is the attempt to make operating systems more "user-friendly," which Tanenbaum views as an excuse for bloated code. The solution is to have smart people, the smallest possible team, and well-defined interactions between various operating systems components. Future operating system design will benefit if the advice in this book is taken to heart. -Pete Ostenson - Dieser Text bezieht sich auf eine vergriffene oder nicht verf{\"{u}}gbare Ausgabe dieses Titels.},
archivePrefix = {arXiv},
arxivId = {0710.2023},
author = {Tanenbaum, Andrew S. and Bos, Herbert},
booktitle = {Education},
doi = {10.1142/S0129183108012261},
eprint = {0710.2023},
isbn = {9780133591620},
issn = {13541013},
pages = {1137},
pmid = {12204814},
publisher = {Pearson Education, Inc.},
title = {{Modern Operating Systems}},
url = {http://www.amazon.com/dp/0136006639},
volume = {2},
year = {2014}
}
@article{18,
abstract = {GPUs are being increasingly adopted as compute accelerators in many domains, spanning environments from mobile systems to cloud computing. These systems are usually running multiple applications, from one or several users. However GPUs do not provide the support for resource sharing traditionally expected in these scenarios. Thus, such systems are unable to provide key multiprogrammed workload requirements, such as responsiveness, fairness or quality of service. In this paper, we propose a set of hardware extensions that allow GPUs to efficiently support multiprogrammed GPU workloads. We argue for preemptive multitasking and design two preemption mechanisms that can be used to implement GPU scheduling policies. We extend the architecture to allow concurrent execution of GPU kernels from different user processes and implement a scheduling policy that dynamically distributes the GPU cores among concurrently running kernels, according to their priorities. We extend the NVIDIA GK110 (Kepler) like GPU architecture with our proposals and evaluate them on a set of multiprogrammed workloads with up to eight concurrent processes. Our proposals improve execution time of high-priority processes by 15.6x, the average application turnaround time between 1.5x to 2x, and system fairness up to 3.4x. {\textcopyright} 2014 IEEE.},
author = {Tanasic, Ivan and Gelado, Isaac and Cabezas, Javier and Ramirez, Alex and Navarro, Nacho and Valero, Mateo},
doi = {10.1109/ISCA.2014.6853208},
isbn = {9781479943968},
issn = {10636897},
journal = {Proceedings - International Symposium on Computer Architecture},
keywords = {concurrency control;graphics processing units;mult},
month = {jun},
pages = {193--204},
title = {{Enabling preemptive multiprogramming on GPUs}},
year = {2014}
}
@article{Gdev,
abstract = {Graphics processing units (GPUs) have become a very powerful platform embracing a concept of heterogeneous many-core computing. However, application domains of GPUs are currently limited to specific systems, largely due to a lack of “first-class” GPU resource management for general-purpose multi-tasking systems. We present Gdev, a new ecosystem of GPU resource management in the operating system (OS). It allows the user space as well as the OS itself to use GPUs as first-class computing resources. Specifically, Gdev's virtual memory manager supports data swapping for excessive memory resource demands, and also provides a shared device memory functionality that allows GPU contexts to communicate with other contexts. Gdev further provides a GPU scheduling scheme to virtualize a physical GPU into multiple logical GPUs, enhancing isolation among working sets of multi-tasking systems. Our evaluation conducted on Linux and the NVIDIA GPU shows that the basic performance of our prototype implementation is reliable even compared to proprietary software. Further detailed experiments demonstrate that Gdev achieves a 2x speedup for an encrypted file system using the GPU in the OS. Gdev can also improve the makespan of dataflow programs by up to 49{\%} exploiting shared device memory, while an error in the utilization of virtualized GPUs can be limited within only 7{\%}.},
author = {Kato, Shinpei and McThrow, Michael and Maltzahn, Carlos and Brandt, Scott},
isbn = {9781931971935},
journal = {Proceedings of the 2012 USENIX Annual Technical Conference, USENIX ATC 2012},
pages = {401--412},
title = {{GDeV: First-class GPU resource management in the operating system}},
year = {2019}
}
@article{ComRTT,
author = {Shinde, Vijayshree and C., Seema},
doi = {10.5120/ijca2017912832},
journal = {International Journal of Computer Applications},
number = {6},
pages = {37--41},
title = {{Comparison of Real Time Task Scheduling Algorithms}},
volume = {158},
year = {2017}
}
@article{IeeeSG,
abstract = {Describes the IEEE Std 610.12-1990, IEEE standard glossary of software engineering terminology, which identifies terms currently in use in the field of software engineering. Standard definitions for those terms are established.},
author = {Ieee},
doi = {10.1109/IEEESTD.1990.101064},
isbn = {155937067X},
journal = {Office},
keywords = {definitions,dictionary,glossary,software engineering,terminology},
month = {dec},
number = {1},
pages = {1},
title = {{IEEE Standard Glossary of Software Engineering Terminology}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=159342},
volume = {121990},
year = {1990}
}
@url{medium,
author = {Priambodo, Bobby},
title = {{Cooperative vs. Preemptive: a quest to maximize concurrency power}},
url = {https://medium.com/traveloka-engineering/cooperative-vs-preemptive-a-quest-to-maximize-concurrency-power-3b10c5a920fe}
}
@url{TOP500,
author = {500, TOP},
booktitle = {TOP 500},
month = {jun},
title = {{Top 500 June 2019}},
url = {https://www.top500.org/lists/2019/06/},
year = {2019}
}
@book{PreeSch,
abstract = {The Internet of Things (IoT) is a new concept in intelligent automation, monitoring, and control. It is the network of home appliances, vehicles, and any other physical devices connected together with sensors, actuators, and displays, which enables these devices to connect and exchange data. It is estimated that in 2017 there were around 9 billion IoT devices in the world and this number is estimated to grow over 30 billion by 2020. The word “things” in IoT usually refers to devices that have unique identifiers, connected to the Internet to exchange information with each other in real time. Such devices have sensors and/or actuators that can be used to collect data about their environments and to monitor and control these environments as required. This chapter presents the basic features of the IoT and describes the commonly used IoT architectures. Additionally, an IoT home project is given in the chapter.},
author = {Ibrahim, Dogan},
booktitle = {Arm-Based Microcontroller Projects Using Mbed},
doi = {10.1016/c2018-0-02627-4},
month = {apr},
publisher = {ELSEVIER SCIENCE {\&} TECHNOLOGY},
title = {{Arm-Based Microcontroller Projects Using Mbed}},
url = {https://www.sciencedirect.com/topics/engineering/preemptive-scheduling},
year = {2019}
}
@article{dejittering,
abstract = {Video jittering occurs when the horizontal lines of video image frames are randomly displaced due to the corruption of synchronization signals or electromagnetic interference during video transmission. Inspired by the recent Bayesian/variational dejittering model of Shen (SIAM J. Appl. Math., vol. 64, pp. 1691-1708, 2004), in the current paper we propose a novel dejittering approach nicknamed 'bake-and-shake.' The bake step is to apply Perona-Malik type nonlinear diffusions to 'melt away' or heat up the jittered video frames, based upon which the shake step is able to optimally estimate the individual line jitters and renormalize the jittered images. Numerical implementation of the bake-and-shake algorithm as well as several computational results are presented. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Kang, Sung Ha and Shen, Jianhong},
doi = {10.1016/j.imavis.2005.09.022},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Bake,Dejittering,Edge preservation,Line jitters,Newton-Raphson,PDE based method,Perona-Malik,Shake,Texture,Total variation},
number = {2},
pages = {143--152},
title = {{Video dejittering by bake and shake}},
volume = {24},
year = {2006}
}
@url{GpuCpu,
author = {NVIDIA},
title = {{NVIDIA sobre la computaci{\'{o}}n de GPU y la diferencia entre GPU y CPU}},
url = {https://la.nvidia.com/object/what-is-gpu-computing-la.html},
year = {2018}
}
@url{AnPasc,
author = {{HURTADO V.}, JUAN PABLO},
title = {{An{\'{a}}lisis a Fondo: Arquitectura Gpu Nvidia Pascal -- Dise{\~{n}}ada Para La Velocidad}},
url = {https://www.ozeros.com/2016/05/analisis-a-fondo-arquitectura-gpu-nvidia-pascal-disenada-para-la-velocidad/},
year = {2016}
}
@url{envytools,
author = {Envytools},
title = {{Tools for people envious of nvidia's blob driver.}},
url = {https://github.com/envytools/envytools}
}
@article{PreeK,
abstract = {As graphics processing units (GPUs) gain adoption as general purpose parallel compute devices, several key problems need to be addressed in order for their use to become more practical and more user friendly. One such problem is special functions designed to execute on GPUs called kernel functions are non-preempt able. Once the kernel is issued to the GPU it will remain there till either execution finishes or it is killed. If the kernel uses all the execution units of the GPU, then no other kernels are able to be executed. This paper proposes a way to apply preemption to the executing kernel function. The kernel at some point in its execution will be able to save its state, halt execution, and free up the GPU's execution units for other kernels to run. After a given amount of time the halted kernel will be able to regain control of the GPU and complete its execution as if it never was halted in the first place. Experimental results have demonstrated the effectiveness of the proposed scheme. {\textcopyright} 2012 IEEE.},
author = {Calhoun, Jon and Jiang, Hai},
doi = {10.1109/SNPD.2012.53},
isbn = {9780769547619},
journal = {Proceedings - 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, SNPD 2012},
keywords = {CUDA,Checkpointing,GPU,Kernel Function,Preemption},
pages = {247--252},
title = {{Preemption of a CUDA kernel function}},
year = {2012}
}
@article{AsFermi,
author = {Hou, Yunqing and Lai, J and Mikushin, D},
journal = {URL: http://code.google.com/p/asfermi/},
title = {{AsFermi: An assembler for the NVIDIA fermi instruction set}},
year = {2011}
}
@article{DynSche,
abstract = {During the last decade we have witnessed a severe change in computing, as processor clock-rates stopped increasing. Thus, the arguable only way to increase processing power is switching to a parallel computing architecture, like the graphics processing unit (GPU). While a GPU offers tremendous processing power, harnessing this power is often difficult. In our research we tackle this issue, providing various components to allow a wider class of algorithms to execute efficiently on the GPU. These efforts include new processing models for dynamic algorithms with various degrees of parallelism, a versatile task scheduler, based on highly efficient work queues which also support dynamic priority scheduling, and efficient dynamic memory management. Our scheduling strategies advance the state-of-the-art algorithms in the field of rendering, visualization, and geometric modeling. In the field of rendering, we provide algorithms that can significantly speed-up image generation, assigning more processing power to the most important image regions. In the field of geometric modeling we provide the first GPU-based grammar evaluation system that can generate and render cities in real-time which otherwise take hours to generate and could not fit into GPU memory. Finally, we show that mesh processing algorithms can be computed significantly faster on the GPU when parallelizing them with advanced scheduling strategies.},
author = {Steinberger, Markus},
doi = {10.1109/MCG.2018.032421659},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
keywords = {GPU,computer graphics,dynamic algorithms,geometric modeling,parallelization},
number = {3},
pages = {119--130},
title = {{On Dynamic Scheduling for the GPU and its Applications in Computer Graphics and beyond}},
volume = {38},
year = {2018}
}
@book{ChkP,
abstract = {Memory exclusion is a powerful tool for optimizing the performance of checkpointing, however it has not been automated completely with low enough overhead. In this paper we present compiler-assisted memory exclusion (CAME), a technique that uses static program analysis to optimize the performance of checkpointing. With the assistance of user-placed directives, the compiler can perform data flow analyses for dead and read-only regions of memory that can be omitted from checkpoints. The result can be a significant reduction in the size of checkpoints, thereby reducing the overhead of checkpointing.},
author = {Plank, J S and Beck, M and Kingsley, G},
booktitle = {IEEE Technical Committee on Operating Systems and Application Environments},
keywords = {checkpointing},
number = {4},
pages = {10--14},
publisher = {IEEE Technical Committee on Operating Systems and Application Environments},
title = {{Compiler-Assisted Memory Exclusion for Fast Checkpointing}},
volume = {7},
year = {1995}
}
@article{GPES,
abstract = {Graphics processing units (GPUs) are being widely used as co-processors in many application domains to accelerate general-purpose workloads that are computationally intensive, known as GPGPU computing. Real-time multi-tasking support is a critical requirement for many emerging GPGPU computing domains. However, due to the asynchronous and non-preemptive nature of GPU processing, in multi-tasking environments, tasks with higher priority may be blocked by lower priority tasks for a lengthy duration. This severely harms the system's timing predictability and is a serious impediment limiting the applicability of GPGPU in many real-time and embedded systems. In this paper, we present an efficient GPGPU preemptive execution system (GPES), which combines user-level and driverlevel runtime engines to reduce the pending time of high-priority GPGPU tasks that may be blocked by long-freezing low-priority competing workloads. GPES automatically slices a long-running kernel execution into multiple subkernel launches and splits data transaction into multiple chunks at user-level, then inserts preemption points between subkernel launches and memorycopy operations at driver-level. We implement a prototype of GPES, and use real-world benchmarks and case studies for evaluation. Experimental results demonstrate that GPES is able to reduce the pending time of high-priority tasks in a multitasking environment by up to 90{\%} over the existing GPU driver solutions, while introducing small overheads.},
author = {Zhou, Husheng and Tong, Guangmo and Liu, Cong},
doi = {10.1109/RTAS.2015.7108420},
isbn = {9781479986033},
issn = {15453421},
journal = {Proceedings of the IEEE Real-Time and Embedded Technology and Applications Symposium, RTAS},
pages = {87--97},
title = {{GPES: A preemptive execution system for GPGPU computing}},
volume = {2015-May},
year = {2015}
}
@article{LimPree,
abstract = {The optimality of the Earliest Deadline First scheduler for uniprocessor systems is one of the main reasons behind the popularity of this algorithm among real-time systems. The ability of fully utilizing the computational power of a processing unit however requires the possibility of preempting a task before its completion. When preemptions are disabled, the schedulability overhead could be significant, leading to deadline misses even at system utilizations close to zero. On the other hand, each preemption causes an increase in the runtime overhead due to the operations executed during a context switch and the negative cache effects resulting from interleaving tasks' executions. These factors have been often neglected in previous theoretical works, ignoring the cost of preemption in real applications. A hybrid limited-preemption real-time scheduling algorithm is derived here, that aims to have low runtime overhead while scheduling all systems that can be scheduled by fully preemptive algorithms. This hybrid algorithm permits preemption where necessary for maintaining feasibility, but attempts to avoid unnecessary preemptions during runtime. The positive effects of this approach are not limited to a reduced runtime overhead, but will be extended as well to a simplified handling of shared resources. {\textcopyright} 2010 IEEE.},
author = {Bertogna, Marko and Baruah, Sanjoy},
doi = {10.1109/TII.2010.2049654},
issn = {15513203},
journal = {IEEE Transactions on Industrial Informatics},
keywords = {Earliest deadline first (EDF),preemption,scheduling,shared resources,sporadic tasks,uniprocessors},
number = {4},
pages = {579--591},
title = {{Limited preemption EDF scheduling of sporadic task systems}},
volume = {6},
year = {2010}
}
@article{Pridriven,
abstract = {Many visual tasks in modern personal devices such smartphones resort heavily to graphics processing units (GPUs) for their fluent user experiences. Because most GPUs for embedded systems are non-preemptive by nature, it is important to schedule GPU resources efficiently across multiple GPU tasks. We present a novel spatial resource sharing (SRS) technique for GPU tasks, called a budget-reservation spatial resource sharing (BR-SRS) scheduling, which limits the number of GPU processing cores for a job based on the priority of the job. Such a priority-driven resource assignment can prevent a high-priority foreground GPU task from being delayed by background GPU tasks. The BR-SRS scheduler is invoked only twice at the arrival and completion of jobs, and thus, the scheduling overhead is minimized as well. We evaluated the performance of our scheduling scheme in an Android-based smartphone, and found that the proposed technique significantly improved the performance of high-priority tasks in comparison to the previous temporal budget-based multi-task scheduling.},
author = {Kang, Yunji and Joo, Woohyun and Lee, Sungkil and Shin, Dongkun},
doi = {10.1016/j.sysarc.2017.04.002},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {Embedded GPU,GPU job scheduling,Resource reservation,Spatial resource sharing},
pages = {17--27},
title = {{Priority-driven spatial resource sharing scheduling for embedded graphics processing units}},
volume = {76},
year = {2017}
}
@article{GPUSync,
abstract = {This paper describes GPUSync, which is a framework for managing graphics processing units (GPUs) in multi-GPU multicore real-time systems. GPUSync was designed with flexibility, predictability, and parallelism in mind. Specifically, it can be applied under either static-or dynamic priority CPU scheduling, can allocate CPUs/GPUs on a partitioned, clustered, or global basis, provides flexible mechanisms for allocating GPUs to tasks, enables task state to be migrated among different GPUs, with the potential of breaking such state into smaller 'chunks', provides migration cost predictors that determine when migrations can be effective, enables a single GPU's different engines to be accessed in parallel, properly supports GPU-related interrupt and worker threads according to the sporadic task model, even when GPU drivers are closed-source, and provides budget policing to the extent possible, given that GPU access is non-preemptive. No prior real-time GPU management framework provides a comparable range of features. {\textcopyright} 2013 IEEE.},
author = {Elliott, Glenn A. and Ward, Bryan C. and Anderson, James H.},
doi = {10.1109/RTSS.2013.12},
isbn = {9781479920075},
issn = {10528725},
journal = {Proceedings - Real-Time Systems Symposium},
keywords = {GPGPU,Operating systems,Real time systems,Schedulability},
pages = {33--44},
title = {{GPUSync: A framework for real-time GPU management}},
year = {2013}
}
@manual{TX2CU,
author = {STANCU, Alexandru and CODRES, Eduard and Guerrero, Mario Martinez},
edition = {Second Edi},
organization = {NVIDIA},
title = {{Jetson TX2 and CUDA Programming}},
year = {2018}
}
@article{RGEM,
abstract = {General-purpose computing on graphics processing units, also known as GPGPU, is a burgeoning technique to enhance the computation of parallel programs. Applying this technique to real-time applications, however, requires additional support for timeliness of execution. In particular, the non-preemptive nature of GPGPU, associated with copying data to/from the device memory and launching code onto the device, needs to be managed in a timely manner. In this paper, we present a responsive GPGPU execution model (RGEM), which is a user-space runtime solution to protect the response times of high-priority GPGPU tasks from competing workload. RGEM splits a memory-copy transaction into multiple chunks so that preemption points appear at chunk boundaries. It also ensures that only the highest-priority GPGPU task launches code onto the device at any given time, to avoid performance interference caused by concurrent launches. A prototype implementation of an RGEM-based CUDA runtime engine is provided to evaluate the real-world impact of RGEM. Our experiments demonstrate that the response times of high-priority GPGPU tasks can be protected under RGEM, whereas their response times increase in an unbounded fashion without RGEM support, as the data sizes of competing workload increase. {\textcopyright} 2011 IEEE.},
author = {Kato, Shinpei and Lakshmanan, Karthik and Kumar, Aman and Kelkar, Mihir and Ishikawa, Yutaka and Rajkumar, Ragunathan},
doi = {10.1109/RTSS.2011.13},
isbn = {9780769545912},
issn = {10528725},
journal = {Proceedings - Real-Time Systems Symposium},
pages = {57--66},
title = {{RGEM: A responsive GPGPU execution model for runtime engines}},
year = {2011}
}
@article{RTFG,
abstract = {Graphics processing units (GPUs) have been employed in the critical path of applications in embedded systems due to the GPUs' programmability, high-performance, and low power consumption. State-of-the-art GPUs have the capability to process multiple GPU workloads concurrently. Moreover, GPU-based embedded systems have been considered to be essential because of the increased number of throughput-oriented applications and system events. However, existing application scheduling frameworks on a GPU do not have enough flexibility to handle the dynamic behavior of the event-driven applications. This is because in the existing scheduling frameworks: 1) only temporal preemption is considered and 2) one application occupies the GPU at a time. In order to tackle these problems, we propose a novel run-time scheduling framework that considers both temporal and spatial preemptions concurrently. We demonstrate the capability and novelty of our framework compared to the existing scheduling frameworks with realistic benchmark applications and with different execution scenarios. Experimental results show that our scheduling framework is able to guarantee up to 1.37 times as many applications compared to other scheduling frameworks. Moreover, the total amount of timing violation is decreased by up to 54.57{\%}.},
author = {Lee, Haeseung and {Al Faruque}, Mohammad Abdullah},
doi = {10.1109/TCAD.2016.2547916},
issn = {02780070},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
keywords = {Embedded system,event-driven application,graphics processing unit (GPU),multicore systems,preemption,run-time scheduling,soft real-time systems},
number = {12},
pages = {1956--1967},
title = {{Run-Time Scheduling Framework for Event-Driven Applications on a GPU-Based Embedded System}},
volume = {35},
year = {2016}
}
@article{DaTransf,
abstract = {Graphics processing units (GPUs) embrace many-core compute devices where massively parallel compute threads are offloaded from CPUs. This heterogeneous nature of GPU computing raises non-trivial data transfer problems especially against latency-critical real-time systems. However even the basic characteristics of data transfers associated with GPU computing are not well studied in the literature. In this paper, we investigate and characterize currently-achievable data transfer methods of cutting-edge GPU technology. We implement these methods using open-source software to compare their performance and latency for real-world systems. Our experimental results show that the hardware-assisted direct memory access (DMA) and the I/O read-and-write access methods are usually the most effective, while on-chip micro controllers inside the GPU are useful in terms of reducing the data transfer latency for concurrent multiple data streams. We also disclose that CPU priorities can protect the performance of GPU data transfers. {\textcopyright} 2013 IEEE.},
author = {Fujii, Yusuke and Azumi, Takuya and Nishio, Nobuhiko and Kato, Shinpei and Edahiro, Masato},
doi = {10.1109/ICPADS.2013.47},
isbn = {9781479920815},
issn = {15219097},
journal = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
keywords = {Data Transfer,GPGPU,Latency,OS,Performance},
pages = {275--282},
title = {{Data transfer matters for GPU computing}},
year = {2013}
}
@inproceedings{Effisha,
abstract = {Modern GPUS are broadly adopted in many multitasking environments, including data centers and smartphones. However, the current support for the scheduling of multiple GPU kernels (from different applications) is limited, forming a major barrier for GPU to meet many practical needs. This work for the first time demonstrates that on existing GPUS, efficient preemptive scheduling of GPU kernels is possible even without special hardware support. Specifically, it presents EffiSha, a pure software framework that enables preemptive scheduling of GPU kernels with very low overhead. The enabled preemptive scheduler offers flexible support of kernels of different priorities, and demonstrates significant potential for reducing the average turnaround time and improving the system overall throughput of programs that time share a modern GPU.},
author = {Chen, Guoyang and Zhao, Yue and Shen, Xipeng and Zhou, Huiyang},
booktitle = {Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
doi = {10.1145/3018743.3018748},
isbn = {9781450344937},
month = {jan},
pages = {3--16},
publisher = {Association for Computing Machinery},
title = {{Effisha: A software framework for enabling efficient preemptive scheduling of GPU}},
year = {2017}
}
@url{PasGPU,
author = {NVIDIA},
title = {{ARQUITECTURA PASCAL DE NVIDIA Computaci{\'{o}}n infinita para oportunidades infinitas}},
url = {https://www.nvidia.com/es-la/data-center/pascal-gpu-architecture/}
}
@url{PasAna,
author = {Smith, Ryan},
booktitle = {AnandTech},
title = {{The NVIDIA GeForce GTX 1080 {\&} GTX 1070 Founders Editions Review : Kicking Off the FinFET Generation}},
url = {http://www.anandtech.com/print/10325/the-nvidia-geforce-gtx-1080-and-1070-founders-edition-review},
year = {2016}
}

@book{CUDAP,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{John Cheng} and {Max Grossman} and {Ty McKercher}},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-118-73932-7},
issn = {1098-6596},
keywords = {icle},
mendeley-groups = {Tesis},
pages = {528},
pmid = {25246403},
title = {{Professional CUDA C Programming}},
year = {2014}
}

@misc{SMJetson,
author = {Franklin, Dustin},
title = {{NVIDIA Jetson TX2 Delivers Twice the Intelligence to the Edge}},
url = {https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/},
year = {2017}
}
